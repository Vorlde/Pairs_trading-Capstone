{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vorlde/Pairs_trading-Reinforcement-Learning/blob/main/Pair_Selection_data_process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "uqSfNjIZoiKr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqSfNjIZoiKr",
        "outputId": "ac965de4-99e6-4f33-e1fe-80800f2779eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dill\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dill\n",
            "Successfully installed dill-0.3.8\n"
          ]
        }
      ],
      "source": [
        "! pip install dill\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from statsmodels.tsa.stattools import coint\n",
        "\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fVokhSQvohfD",
      "metadata": {
        "id": "fVokhSQvohfD"
      },
      "outputs": [],
      "source": [
        "import lzma\n",
        "import dill as pickle\n",
        "\n",
        "def save_pickle(path,obj):\n",
        "    with lzma.open(path,\"wb\") as fp:\n",
        "        pickle.dump(obj,fp)\n",
        "\n",
        "def load_pickle(path):\n",
        "    with lzma.open(path,\"rb\") as fp:\n",
        "        file = pickle.load(fp)\n",
        "    return file\n",
        "\n",
        "\n",
        "def clean_data(ticker_dfs,tickers):\n",
        "    intraday_range = ticker_dfs[tickers[0]].index\n",
        "    for inst in tickers:\n",
        "        ticker_dfs[inst] = ticker_dfs[inst].reindex(intraday_range)\n",
        "    closes = []\n",
        "\n",
        "    for tk in tickers:\n",
        "        close = ticker_dfs[tk].close\n",
        "        closes.append(close)\n",
        "\n",
        "    pricing = pd.concat(closes,axis = 1)\n",
        "    pricing.columns = tickers\n",
        "\n",
        "    return pricing\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_pca_features(ret_df,N_PRIN_COMPONENTS =10):\n",
        "\n",
        "    pca = PCA(n_components=N_PRIN_COMPONENTS)\n",
        "    pca.fit(ret_df)\n",
        "\n",
        "    # Extract factor loadings\n",
        "    factor_loadings = pca.components_.T  # Transpose the components matrix\n",
        "\n",
        "    # Create a DataFrame with the correct orientation\n",
        "    factor_loadings_df = pd.DataFrame(factor_loadings, index=ret_df.columns, columns=[f'Factor {i+1}' for i in range(N_PRIN_COMPONENTS)])\n",
        "\n",
        "    X = preprocessing.StandardScaler().fit_transform(pca.components_.T)\n",
        "\n",
        "    return X\n",
        "\n",
        "def create_clusters(X,index):\n",
        "    clf = DBSCAN(eps=1, min_samples=3)\n",
        "    print(clf)\n",
        "\n",
        "    clf.fit(X)\n",
        "    labels = clf.labels_\n",
        "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "    print(\"\\nClusters discovered: %d\" % n_clusters_)\n",
        "\n",
        "    clustered = clf.labels_\n",
        "\n",
        "    clustered_series = pd.Series(index=index, data=clustered.flatten())\n",
        "    clustered_series = clustered_series[clustered_series != -1]\n",
        "\n",
        "    return clustered_series\n",
        "\n",
        "\n",
        "def find_cointegrated_pairs(data, significance=0.05):\n",
        "    # This function is from https://www.quantopian.com/lectures/introduction-to-pairs-trading\n",
        "    n = data.shape[1]\n",
        "    score_matrix = np.zeros((n, n))\n",
        "    pvalue_matrix = np.ones((n, n))\n",
        "    keys = data.keys()\n",
        "    pairs = []\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            S1 = data[keys[i]]\n",
        "            S2 = data[keys[j]]\n",
        "            result = coint(S1, S2)\n",
        "            score = result[0]\n",
        "            pvalue = result[1]\n",
        "            score_matrix[i, j] = score\n",
        "            pvalue_matrix[i, j] = pvalue\n",
        "            if pvalue < significance:\n",
        "                pairs.append((keys[i], keys[j]))\n",
        "    return score_matrix, pvalue_matrix, pairs\n",
        "\n",
        "\n",
        "\n",
        "def get_coint_pairs(prices,clustered_series):\n",
        "\n",
        "    CLUSTER_SIZE_LIMIT = 9999\n",
        "    counts = clustered_series.value_counts()\n",
        "    ticker_count_reduced = counts[(counts>1) & (counts<=CLUSTER_SIZE_LIMIT)]\n",
        "\n",
        "    cluster_dict = {}\n",
        "    for i, which_clust in enumerate(ticker_count_reduced.index):\n",
        "        tickers = clustered_series[clustered_series == which_clust].index\n",
        "        score_matrix, pvalue_matrix, pairs = find_cointegrated_pairs(\n",
        "            prices[tickers]\n",
        "        )\n",
        "        cluster_dict[which_clust] = {}\n",
        "        cluster_dict[which_clust]['score_matrix'] = score_matrix\n",
        "        cluster_dict[which_clust]['pvalue_matrix'] = pvalue_matrix\n",
        "        cluster_dict[which_clust]['pairs'] = pairs\n",
        "\n",
        "    pairs = []\n",
        "    for clust in cluster_dict.keys():\n",
        "        pairs.extend(cluster_dict[clust]['pairs'])\n",
        "\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    data_path = \"/content/drive/My Drive/constituents.csv\"\n",
        "    dfs_path = \"/content/drive/My Drive/new_dfs.obj\"\n",
        "\n",
        "    ticker_dfs = load_pickle(dfs_path)\n",
        "    snp_data = pd.read_csv(data_path)\n",
        "    tickers = []\n",
        "\n",
        "    for i in range(499):\n",
        "      tickers.append(snp_data.Symbol[i])\n",
        "\n",
        "    tickers.remove(\"BF.B\")\n",
        "    tickers.remove(\"BRK.B\")\n",
        "    tickers.remove(\"CPAY\")\n",
        "    tickers.remove(\"DAY\")\n",
        "    tickers.remove(\"GEV\")\n",
        "    tickers.remove(\"SOLV\")\n",
        "\n",
        "    return tickers,ticker_dfs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_pair_and_store(pair, prices, h5_file_path):\n",
        "\n",
        "    # print(f\"processing {pair}\")\n",
        "    s1, s2 = pair\n",
        "    # datax = prices[s1]\n",
        "    # datay = prices[s2]\n",
        "\n",
        "    # price1_train = datax[:600]\n",
        "    # price2_train = datay[:600]\n",
        "\n",
        "    # # Conduct OLS regression to find the hedge ratio\n",
        "    # results = sm.OLS(price1_train, sm.add_constant(price2_train)).fit()\n",
        "    # hedge_ratio = results.params[1]  # The coefficient (beta) for price2\n",
        "\n",
        "    # # Create a DataFrame with the features you want to use\n",
        "    # # For example, hedge_ratio could be one of the features\n",
        "    # features_df = pd.DataFrame({\n",
        "    #     'hedge_ratio': hedge_ratio,\n",
        "    #     # Add more features here\n",
        "    # })\n",
        "\n",
        "    # # Store this DataFrame in the h5py file\n",
        "    # with h5py.File(h5_file_path, 'a') as h5f:\n",
        "    #     group = h5f.require_group('cointegrated_pairs')\n",
        "    #     dataset_name = f\"{s1}_{s2}\"\n",
        "    #     # If dataset already exists, delete it before creating a new one\n",
        "    #     if dataset_name in group:\n",
        "    #         del group[dataset_name]\n",
        "    #     group.create_dataset(dataset_name, data=features_df.values)\n",
        "\n",
        "\n",
        "    # import statsmodels.api as sm\n",
        "    # from statsmodels.tsa.stattools import adfuller as ADF_test\n",
        "    # import matplotlib.pyplot as plt\n",
        "\n",
        "    # datax = prices[s1]\n",
        "    # datay = prices[s2]\n",
        "\n",
        "    # price1_train = datax[:550]\n",
        "    # price2_train = datay[:550]\n",
        "    # price1_test = datax[550:]\n",
        "    # price2_test = datay[550:]\n",
        "\n",
        "    # # Conduct OLS regression to find the hedge ratio\n",
        "    # # Adding an intercept to model: price1 = beta * price2 + alpha\n",
        "    # results = sm.OLS(price1_train, sm.add_constant(price2_train)).fit()\n",
        "    # hedge_ratio = results.params[1]  # The coefficient (beta) for price2\n",
        "\n",
        "    # # Calculate the spread using the hedge ratio for both training and test data\n",
        "    # spread_train = price1_train - hedge_ratio * price2_train\n",
        "    # spread_test = price1_test - hedge_ratio * price2_test\n",
        "    # full_spread = pd.concat([spread_train, spread_test]).reset_index(drop=True)\n",
        "\n",
        "    # # Perform the Augmented Dickey-Fuller (ADF) test on the training spread\n",
        "    # adf_result = ADF_test(spread_train)\n",
        "\n",
        "    # # Check if the training spread is stationary\n",
        "    # if adf_result[1] < 0.5:  # ADF test returns a tuple where the second item is p-value\n",
        "    #     print(ADF_test(spread_train)[1])\n",
        "    #     print(ADF_test(spread_test)[1])\n",
        "\n",
        "    #     # Calculate the mean and standard deviation of the full spread\n",
        "    #     mu = full_spread.mean()\n",
        "    #     sigma = full_spread.std()\n",
        "\n",
        "    #     # Calculate the Z-scores of the full spread\n",
        "    #     z_scores = (full_spread - mu) / sigma\n",
        "\n",
        "    #     # Plotting the spread and Z-scores for full dataset\n",
        "    #     plt.figure(figsize=(15, 7))\n",
        "    #     plt.title(f'Z-Scores for {s1} and {s2} Across Training and Testing Periods')\n",
        "    #     z_scores.plot()\n",
        "    #     plt.axvline(x=600, color='purple', linestyle='--', label='Training/Test Split')\n",
        "    #     plt.axhline(z_scores.mean(), color='black', label='Mean')  # Mean line\n",
        "    #     plt.axhline(2, color='red', linestyle='--', label='+2 Std Dev')  # Upper threshold\n",
        "    #     plt.axhline(-2, color='green', linestyle='--', label='-2 Std Dev')  # Lower threshold\n",
        "    #     plt.legend()\n",
        "    #     plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "jM-J697NAl5U"
      },
      "id": "jM-J697NAl5U",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "dsXXhxNZoMP7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsXXhxNZoMP7",
        "outputId": "3fa4a147-51ae-499b-eef6-28efd0af597c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "tickers,ticker_dfs = get_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_trading_data_blocks(df, intervals_per_day=78):\n",
        "    \"\"\"\n",
        "    Generate blocks of data consisting of 9 trading days for each month.\n",
        "    \"\"\"\n",
        "    j = 0\n",
        "    for year in df.index.year.unique():\n",
        "        for month in df[df.index.year == year].index.month.unique():\n",
        "            monthly_data = df[(df.index.year == year) & (df.index.month == month)]\n",
        "            trading_days = monthly_data.index.day.unique()\n",
        "\n",
        "            # Determine the block indices\n",
        "            block_start_indices = [i * intervals_per_day * 9 for i in range((len(trading_days) + 8) // 9)]\n",
        "            for start_day in block_start_indices:\n",
        "                # Calculate the block's end day, being careful not to exceed the available days\n",
        "                end_day = min(start_day + 9, len(trading_days))\n",
        "                block_days = trading_days[start_day:end_day]\n",
        "                # Yield the data for these specific trading days\n",
        "                block_data = monthly_data[monthly_data.index.day.isin(block_days)]\n",
        "                yield j, year, month, block_data\n",
        "                j+=1\n",
        "\n",
        "\n",
        "def generate_trading_data_blocks(df):\n",
        "    # Define the number of intervals per trading day\n",
        "    intervals_per_day = 78\n",
        "    j = 0\n",
        "\n",
        "    for year in df.index.year.unique():\n",
        "        yearly_data = df[df.index.year == year]\n",
        "        for month in yearly_data.index.month.unique():\n",
        "            monthly_data = yearly_data[yearly_data.index.month == month]\n",
        "\n",
        "            # Print statement for where clustering code will go\n",
        "            print(f\"Clustering code for {year}-{month} goes here.\")\n",
        "\n",
        "            # Extract the unique days in the monthly data\n",
        "            trading_days = monthly_data.index.day.unique()\n",
        "\n",
        "            # Initialize the start index for the first block\n",
        "            start_idx = 0\n",
        "            end_idx = intervals_per_day * 9  # 9 days worth of intervals\n",
        "\n",
        "            # Generate two blocks if possible\n",
        "            for _ in range(2):\n",
        "                # Ensure we have enough data for a full block\n",
        "                if end_idx <= len(monthly_data):\n",
        "                    block_data = monthly_data.iloc[start_idx:end_idx]\n",
        "                    yield j, year, month, block_data\n",
        "                    start_idx = end_idx\n",
        "                    end_idx += intervals_per_day * 9\n",
        "                    j+=1\n",
        "\n",
        "def generate_trading_data_blocks(df, days_per_block=9, step_days=5):\n",
        "    \"\"\"\n",
        "    Generate rolling blocks of data consisting of 9 trading days,\n",
        "    advancing the window by 5 days after each block from the original dataframe.\n",
        "    \"\"\"\n",
        "    # Extract unique trading days from the index\n",
        "    unique_days = df.index.normalize().unique()\n",
        "\n",
        "    for start_day_idx in range(0, len(unique_days) - days_per_block + 1, step_days):\n",
        "        # Identify start and end day for the block\n",
        "        start_day = unique_days[start_day_idx]\n",
        "        end_day_idx = start_day_idx + days_per_block - 1  # inclusive end day index\n",
        "        if end_day_idx >= len(unique_days):  # Ensure we don't go out of bounds\n",
        "            break\n",
        "        end_day = unique_days[end_day_idx]\n",
        "\n",
        "        # Slice the original dataframe to get the block data\n",
        "        block_data = df[start_day:end_day]\n",
        "        yield block_data\n",
        "\n",
        "\n",
        "def calculate_monthly_clusters(ret_df):\n",
        "    \"\"\"\n",
        "    Calculate clusters for the given month.\n",
        "    \"\"\"\n",
        "    X = get_pca_features(ret_df)\n",
        "    clustered_series = create_clusters(X, ret_df.columns)\n",
        "\n",
        "    return clustered_series\n"
      ],
      "metadata": {
        "id": "T7_Lli9kyIlD"
      },
      "id": "T7_Lli9kyIlD",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main driver function to integrate the generation of trading blocks, cluster calculation, and pair processing\n",
        "def main_driver_function(prices, h5_file_path):\n",
        "\n",
        "    for block_data in generate_trading_data_blocks(prices):\n",
        "      if block_data.shape[0] ==624:\n",
        "        print(block_data.shape[0])\n",
        "\n",
        "        block_data = block_data.dropna(axis=1)\n",
        "\n",
        "        input(block_data.shape)\n",
        "\n",
        "        # ret_df = block_data.pct_change().round(4).fillna(0)\n",
        "        # cluster = calculate_monthly_clusters(ret_df)\n",
        "\n",
        "        # if j!=0:\n",
        "        #   print(f\"going for {year}, {month}\")\n",
        "        #   pairs = get_coint_pairs(block_data, cluster)\n",
        "        #   process_pair_and_store(pairs, block_data, h5_file_path)\n",
        "\n",
        "\n",
        "h5_file_path = 'path_to_h5_file.h5'  # Define the correct path\n",
        "intraday_dfs = ticker_dfs.copy()\n",
        "prices = clean_data(intraday_dfs,tickers)\n",
        "main_driver_function(prices[-3000:], 'path_to_h5_file.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "sZ7bcnCgojuy",
        "outputId": "d668720e-e4f8-4306-9247-f421c4885422"
      },
      "id": "sZ7bcnCgojuy",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "624\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-a2169f0edfc1>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mintraday_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mticker_dfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintraday_dfs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmain_driver_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'path_to_h5_file.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-a2169f0edfc1>\u001b[0m in \u001b[0;36mmain_driver_function\u001b[0;34m(prices, h5_file_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mblock_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# ret_df = block_data.pct_change().round(4).fillna(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OONRwvoP1pIm"
      },
      "id": "OONRwvoP1pIm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}